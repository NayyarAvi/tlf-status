{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1b21ee-55db-4416-87fe-02bb0be5321f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, BooleanType, TimestampType, LongType\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39bc1c1e-02e4-44c2-9282-f287c99b028a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Initializes and returns a Spark session.\"\"\"\n",
    "\n",
    "def init_spark_session():\n",
    "    try:\n",
    "        return SparkSession.builder.appName('TfL Lines Data').getOrCreate()\n",
    "    except Exception as err:\n",
    "        raise RuntimeError(f\"Error initializing Spark session: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d083519-2195-4529-b6d5-618f71b06651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining schemas for nested json structures\n",
    "\"\"\"\n",
    "def define_schemas():\n",
    "    try:\n",
    "        validity_periods_schema = StructType([\n",
    "            StructField(\"$type\", StringType(), True),\n",
    "            StructField(\"fromDate\", StringType(), True),\n",
    "            StructField(\"toDate\", StringType(), True),\n",
    "            StructField(\"isNow\", BooleanType(), True)\n",
    "        ])\n",
    "\n",
    "        disruption_schema = StructType([\n",
    "            StructField(\"$type\", StringType(), True),\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"categoryDescription\", StringType(), True),\n",
    "            StructField(\"description\", StringType(), True),\n",
    "            StructField(\"affectedRoutes\", ArrayType(StringType()), True),\n",
    "            StructField(\"affectedStops\", ArrayType(StringType()), True),\n",
    "            StructField(\"closureText\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        line_statuses_schema = StructType([\n",
    "            StructField(\"$type\", StringType(), True),\n",
    "            StructField(\"id\", LongType(), True),\n",
    "            StructField(\"lineId\", StringType(), True),\n",
    "            StructField(\"statusSeverity\", LongType(), True),\n",
    "            StructField(\"statusSeverityDescription\", StringType(), True),\n",
    "            StructField(\"reason\", StringType(), True),\n",
    "            StructField(\"created\", StringType(), True),\n",
    "            StructField(\"validityPeriods\", ArrayType(validity_periods_schema), True),\n",
    "            StructField(\"disruption\", disruption_schema, True)\n",
    "        ])\n",
    "\n",
    "        service_types_schema = StructType([\n",
    "            StructField(\"$type\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"uri\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        crowding_schema = StructType([\n",
    "            StructField(\"$type\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        line_schema = StructType([\n",
    "            StructField(\"$type\", StringType(), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"modeName\", StringType(), True),\n",
    "            StructField(\"disruptions\", ArrayType(StringType()), True),\n",
    "            StructField(\"created\", StringType(), True),\n",
    "            StructField(\"modified\", StringType(), True),\n",
    "            StructField(\"lineStatuses\", ArrayType(line_statuses_schema), True),\n",
    "            StructField(\"routeSections\", ArrayType(StringType()), True),\n",
    "            StructField(\"serviceTypes\", ArrayType(service_types_schema), True),\n",
    "            StructField(\"crowding\", crowding_schema, True)\n",
    "        ])\n",
    "\n",
    "        return line_schema\n",
    "    except ValueError as err:\n",
    "        raise ValueError(f\"Error defining schemas: {err}\")\n",
    "    except Exception as err:\n",
    "        raise Exception(f\"Exception defining schemas: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31be9df-b27e-44aa-90b2-b521554ad17d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Fetching the JSON data from the API.\n",
    "2. Storing the data in a temporary file.\n",
    "3. Moving the data from the temporary file location to a DFFS location.\n",
    "\"\"\"\n",
    "\n",
    "def fetch_and_store_data(url, temp_file_path, file_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "        json_data = response.json()\n",
    "\n",
    "        with open(temp_file_path, \"w\") as file:\n",
    "            json.dump(json_data, file)\n",
    "\n",
    "        dbutils.fs.cp('file://' + temp_file_path, file_path)\n",
    "\n",
    "        print(f\"Data successfully fetched from {url} and stored at {file_path}\")\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        raise ValueError(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        raise ConnectionError(f\"Request error: {req_err}\")\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        raise ValueError(f\"Error decoding JSON: {json_err}\")\n",
    "    except IOError as io_err:\n",
    "        raise IOError(f\"File operation error: {io_err}\")\n",
    "    except Exception as err:\n",
    "        raise Exception(f\"An unexpected error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0946ceb2-81cd-4692-8410-ad66436bf314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 1. reading the incoming json data from API and converting it into a dataframe\n",
    " 2. selecting necessary columns (name, lineStatuses) from the dataframe required for further logic\n",
    " 2. adding a current_timestamp column to get the current timestamp\n",
    " 3. exploding lineStatuses column, to fetch the inside key and values\n",
    "\"\"\"\n",
    "\n",
    "def read_and_transform_data(spark, file_path, line_schema):\n",
    "    try:\n",
    "        input_df = spark.read.schema(line_schema).json(file_path, multiLine=True)\n",
    "        \n",
    "        transformed_df = input_df \\\n",
    "            .select(col('name'), col('lineStatuses')) \\\n",
    "            .withColumn('current_timestamp', current_timestamp()) \\\n",
    "            .withColumn(\"exploded_lineStatuses\", explode(col('lineStatuses'))) \\\n",
    "            .drop(col('lineStatuses'))\n",
    "        \n",
    "        print(f\"Data at {file_path}, successfully read and transformed\")\n",
    "\n",
    "        return transformed_df\n",
    "    except ValueError as err:\n",
    "        raise ValueError(f\"Error during data transformation: {err}\")\n",
    "    except Exception as err:\n",
    "        raise Exception(f\"Exception during data transformation: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8964477d-3806-4088-b9ad-3b34a174efdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates and returns the final DataFrame with the desired columns and format.\n",
    "\"\"\"\n",
    "\n",
    "def create_final_dataframe(transformed_df):\n",
    "    try:\n",
    "        final_df = transformed_df.select(\n",
    "            col('current_timestamp'),\n",
    "            col('name').alias('line'),\n",
    "            col(\"exploded_lineStatuses.statusSeverityDescription\").alias(\"status\"),\n",
    "            col(\"exploded_lineStatuses.reason\").alias(\"disruption_reason\")\n",
    "        )\n",
    "        print(f\"Final DataFrame created with desired columns and format\")\n",
    "        return final_df\n",
    "    except ValueError as err:\n",
    "        raise ValueError(f\"Error creating final DataFrame: {err}\")\n",
    "    except Exception as err:\n",
    "        raise Exception(f\"Exception creating final DataFrame: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250716eb-9389-4255-b72a-9e26cfc7a6f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving the final DataFrame in Delta format to an external location.\n",
    "\"\"\"\n",
    "\n",
    "def save_data_as_delta(final_df, delta_path):\n",
    "    try:\n",
    "        final_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        print(f\"Data successfully saved in a delta format at a external location: {delta_path}\")\n",
    "    except IOError as err:\n",
    "        raise IOError(f\"Error saving data as Delta format: {err}\")\n",
    "    except Exception as err:\n",
    "        raise Exception(f\"Exception saving data as Delta format: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d743d8-3545-4bc9-b32c-0aa381b360eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a Delta table using Spark SQL and fetching data from the external location defined above\n",
    "\"\"\"\n",
    "\n",
    "def create_delta_table(spark, delta_path):\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS line_statuses_table (\n",
    "            current_timestamp TIMESTAMP,\n",
    "            line STRING,\n",
    "            status STRING,\n",
    "            disruption_reason STRING\n",
    "        )\n",
    "        USING DELTA\n",
    "        LOCATION '{delta_path}'\n",
    "        \"\"\")\n",
    "        print(f\"Delta Table successfully created using an external location: {delta_path}\")\n",
    "    except Exception as err:\n",
    "        raise RuntimeError(f\"Error creating Delta table: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db938005-b56a-4745-8102-244d046bedd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    spark = init_spark_session()\n",
    "    line_schema = define_schemas()\n",
    "    \n",
    "    url = 'https://api.tfl.gov.uk/Line/Mode/tube/Status'\n",
    "    temp_file_path = '/tmp/input_data.json'\n",
    "    file_path = '/FileStore/tables/input_data.json'\n",
    "    delta_path = \"/FileStore/external/\"\n",
    "    \n",
    "    fetch_and_store_data(url, temp_file_path, file_path)\n",
    "    \n",
    "    transformed_df = read_and_transform_data(spark, file_path, line_schema)\n",
    "    \n",
    "    final_df = create_final_dataframe(transformed_df)\n",
    "    final_df.display()  # Displaying the DataFrame for verification\n",
    "    \n",
    "    save_data_as_delta(final_df, delta_path)\n",
    "    \n",
    "    create_delta_table(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b69977-df91-401d-9fea-037ea6480408",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully fetched from https://api.tfl.gov.uk/Line/Mode/tube/Status and stored at /FileStore/tables/input_data.json\nData at /FileStore/tables/input_data.json, successfully read and transformed\nFinal DataFrame created with desired columns and format\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_timestamp</th><th>line</th><th>status</th><th>disruption_reason</th></tr></thead><tbody><tr><td>2024-07-03T15:16:44.807+0000</td><td>Bakerloo</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Central</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Circle</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>District</td><td>Part Suspended</td><td>District Line: No service between Turnham Green and Richmond while Network Rail fix a track fault at Gunnersbury. Tickets will be accepted on London Buses and South West Rail services via any reasonable route. GOOD SERVICE on the rest of the line. </td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Hammersmith & City</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Jubilee</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Metropolitan</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Northern</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Piccadilly</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Victoria</td><td>Good Service</td><td>null</td></tr><tr><td>2024-07-03T15:16:44.807+0000</td><td>Waterloo & City</td><td>Good Service</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-07-03T15:16:44.807+0000",
         "Bakerloo",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Central",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Circle",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "District",
         "Part Suspended",
         "District Line: No service between Turnham Green and Richmond while Network Rail fix a track fault at Gunnersbury. Tickets will be accepted on London Buses and South West Rail services via any reasonable route. GOOD SERVICE on the rest of the line. "
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Hammersmith & City",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Jubilee",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Metropolitan",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Northern",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Piccadilly",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Victoria",
         "Good Service",
         null
        ],
        [
         "2024-07-03T15:16:44.807+0000",
         "Waterloo & City",
         "Good Service",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "current_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "line",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "disruption_reason",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved in a delta format at a external location: /FileStore/external/\nDelta Table successfully created using an external location: /FileStore/external/\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tlf_status_check",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
